

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Execute a computation &mdash; Documentation for the nGraph Library and Compiler Stack</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  
  
<!-- <link href="https://fonts.googleapis.com/css?family=Nunito:300,300i,400&display=swap&subset=latin-ext" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Nunito+Sans:300,400,600,700,800,900" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">  

  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Documentation for the nGraph Library and Compiler Stack" href="../../index.html"/>
        <link rel="up" title="Constructing Graphs" href="index.html"/>
        <link rel="next" title="Build a graph with operators" href="operator.html"/>
        <link rel="prev" title="Constructing Graphs" href="index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>


<body> 
  <div id="menu-float" class="menu-float">
    <a href="https://www.ngraph.ai" target="_blank"><i class="fa fa-home"></i></a>
    <a href="https://ngraph.nervanasys.com/docs/latest" title="Documentation Home"><i class="fa fa-book"></i></a>
    <a href="https://www.ngraph.ai/tutorials" title="Tutorials"><i class="fa fa-user-circle"></i></a>
    <a href="https://www.youtube.com/embed/C9S0nmNS8bQ" target="_blank"><i class="fa fa-video-camera"></i></a>
    <a href="https://ngraph.slack.com/" title="nGraph Slack Channel"><i class="fa fa-slack"></i></a>
    <a href="https://github.com/NervanaSystems/ngraph/blob/master/LICENSE"><img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"></a>
    <a href="https://www.github.com/NervanaSystems/ngraph"><img src="https://travis-ci.org/NervanaSystems/ngraph.svg?branch=master"></a></div></body>


<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html">
          

          
            
            <br/><img src="../../_static/logo.png" class="logo" />
	    nGraph Compiler Stack
          
          </a>

          
            
            
              <div class="version">
                0.29
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search nGraph Documentation" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

    <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="docvs">nGraph Compiler stack</span>
      v: 0.29
      <span></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Recent Versions<i class="fa fa-terminal"></i></dt>
        <dd><!-- Until our https://docs.ngraph.ai/ publishing is set up, we link to GitHub -->  
          <ul>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.27.1-rc.1">0.27.1</a></li> 
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.27.0-rc.1">0.27.0</a></li> 
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.26.0">0.26.0</a></li>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.25.1-rc.10">0.25.1</a></li>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.25.0">0.25.0</a></li>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.24.0">0.24.0</a></li>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.22.2-rc.0">0.22.2</a></li>
           <li><a href="https://github.com/NervanaSystems/ngraph/releases/tag/v0.22.1">0.22.1</a></li>
         </ul></dd>
      </dl>
      <dl>
        <dt>Links</dt>
          <dd>
           <a href="https://www.ngraph.ai/">Project Home</a>
          </dd>
          <dd>
            <a href="https://github.com/NervanaSystems/ngraph/releases">All Releases</a>
          </dd>
      </dl>
    </div>
</div>


        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            <span class="toctree-expand"><ul>
<li class="toctree-l1"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
</ul>
<p class="caption"><span class="caption-text">Framework Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../frameworks/overview.html">Basic concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../frameworks/tensorflow_connect.html">TensorFlow*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../frameworks/onnx_integ.html">ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../frameworks/paddle_integ.html">PaddlePaddle*</a></li>
</ul>
<p class="caption"><span class="caption-text">nGraph Core</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Basic Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../buildlb.html">Build and Test</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Constructing Graphs</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Execute a computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="operator.html">Build a graph with operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="update.html">Make a stateful computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="derive-for-training.html">Derive a trainable model</a></li>
<li class="toctree-l2"><a class="reference internal" href="distribute-train.html">Distribute training across multiple nGraph backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="import.html">Import a model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/index.html">Using the Python API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../passes/passes.html">Compiler Passes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fusion/index.html">Pattern Matcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ops/index.html">nGraph Core Ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../provenance/index.html">Provenance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dynamic/index.html">Dynamic Shapes</a></li>
</ul>
<p class="caption"><span class="caption-text">Backend Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../backends/index.html">Basic Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends/cpp-api.html">Adding New Backends</a></li>
</ul>
<p class="caption"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/index.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/qat.html">Quantization-Aware Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Validated Workloads</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../frameworks/validated/list.html">Validated Workloads</a></li>
</ul>
<p class="caption"><span class="caption-text">Diagnostics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/debug_core.html">Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/debug_tf.html">Debug TensorFlow*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/debug_onnx.html">Debug ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/debug_paddle.html">Debug PaddlePaddle*</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/viz_tools.html">General Visualization Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../inspection/profiling.html">Performance testing with <code class="docutils literal notranslate"><span class="pre">nbench</span></code></a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../project/contribution-guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a></li>
</ul>
</span>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">nGraph Compiler Stack</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
        <li><a href="index.html">Constructing Graphs</a> &raquo;</li>
      
    <li>Execute a computation</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="../../_sources/core/constructing-graphs/execute.rst.txt" rel="nofollow"> View page source</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="execute-a-computation">
<h1>Execute a computation<a class="headerlink" href="#execute-a-computation" title="Permalink to this headline">¶</a></h1>
<p>This section explains how to manually perform the steps that would normally be
performed by a framework <a class="reference internal" href="../../glossary.html#term-bridge"><span class="xref std std-term">bridge</span></a> to execute a computation. nGraph graphs
are targeted toward automatic construction; it is far easier for a processor
(a CPU, GPU, or <a class="reference external" href="https://www.intel.ai/nervana-nnp">purpose-built silicon</a>) to execute a computation than it is
for a human to map out how that computation happens. Unfortunately, things
that make by-hand graph construction simpler tend to make automatic construction
more difficult, and vice versa.</p>
<p>Nevertheless, it can be helpful to break down what is happening during graph
construction. The documetation that follows explains two approaches frameworks
can use to compile with nGraph operations:</p>
<ul class="simple">
<li><a class="reference internal" href="#scenario-one"><span class="std std-ref">Using complete shapes</span></a></li>
<li><a class="reference internal" href="#scenario-two"><span class="std std-ref">Using partial shapes</span></a></li>
</ul>
<p>The nGraph <abbr title="IR">Intermediate Representation</abbr> uses a strong, dynamic
type system, including static shapes. This means that at compilation, every
tensor (or, equivalently, every node output) in the graph is assigned
<strong>complete shape information</strong>; that is, one and only one shape. The static
process by which this assignment takes place is called <a class="reference internal" href="../../glossary.html#term-shape-propagation"><span class="xref std std-term">shape propagation</span></a>.</p>
<p>In the <a class="reference internal" href="#scenario-one"><span class="std std-ref">first scenario</span></a>, the <a class="reference internal" href="../../glossary.html#term-model-description"><span class="xref std std-term">model description</span></a>
walk-through is based on the <code class="file docutils literal notranslate"><span class="pre">abc.cpp</span></code> code in the <code class="docutils literal notranslate"><span class="pre">/doc/examples/abc</span></code>
directory, and it deconstructs the steps that must happen (either programmatically
or manually) in order to successfully execute a computation given complete
shape information.</p>
<div class="section" id="scenario-one-using-complete-shapes">
<span id="scenario-one"></span><h2>Scenario One: Using Complete Shapes<a class="headerlink" href="#scenario-one-using-complete-shapes" title="Permalink to this headline">¶</a></h2>
<p>A step-by-step example of how a framework might execute with complete shape
information is provided here. For a step-by-step example using dynamic
shapes, see <a class="reference internal" href="#scenario-two"><span class="std std-ref">Scenario Two: Known Partial Shape</span></a>.</p>
<ul class="simple">
<li><a class="reference internal" href="#define-cmp"><span class="std std-ref">Define the computation</span></a></li>
<li><a class="reference internal" href="#specify-backend"><span class="std std-ref">Specify the backend upon which to run the computation</span></a></li>
<li><a class="reference internal" href="#compile-cmp"><span class="std std-ref">Compile the computation</span></a></li>
<li><a class="reference internal" href="#allocate-backend-storage"><span class="std std-ref">Allocate backend storage for the inputs and outputs</span></a></li>
<li><a class="reference internal" href="#initialize-inputs"><span class="std std-ref">Initialize the inputs</span></a></li>
<li><a class="reference internal" href="#invoke-cmp"><span class="std std-ref">Invoke the computation</span></a></li>
<li><a class="reference internal" href="#access-outputs"><span class="std std-ref">Access the outputs</span></a></li>
</ul>
<div class="section" id="define-the-computation">
<span id="define-cmp"></span><h3>Define the computation<a class="headerlink" href="#define-the-computation" title="Permalink to this headline">¶</a></h3>
<p>To a <a class="reference internal" href="../../glossary.html#term-framework"><span class="xref std std-term">framework</span></a>, a computation is simply a transformation of inputs to
outputs. While a <a class="reference internal" href="../../glossary.html#term-bridge"><span class="xref std std-term">bridge</span></a> can programmatically construct the graph
from a framework’s representation of the computation, graph construction can be
somewhat more tedious when done manually. For anyone interested in specific
nodes (vertices) or edges of a computation that reveal “what is happening where”,
it can be helpful to think of a computation as a zoomed-out and <em>stateless</em>
<a class="reference internal" href="../../glossary.html#term-data-flow-graph"><span class="xref std std-term">data-flow graph</span></a> where all of the nodes are well-defined tensor
operations and all of the edges denote use of an output from one operation as an
input for another operation.</p>
<p>Most of the public portion of the nGraph API is in the <code class="docutils literal notranslate"><span class="pre">ngraph</span></code> namespace, so
we will omit the namespace. Use of namespaces other than <code class="docutils literal notranslate"><span class="pre">std</span></code> will be
namespaces in <code class="docutils literal notranslate"><span class="pre">ngraph</span></code>. For example, the <code class="docutils literal notranslate"><span class="pre">op::Add</span></code> is assumed to refer to
<code class="docutils literal notranslate"><span class="pre">ngraph::op::Add</span></code>. A computation’s graph is constructed from ops; each is a
member of a subclass of <code class="docutils literal notranslate"><span class="pre">op::Op</span></code>, which, in turn, is a subclass of <code class="docutils literal notranslate"><span class="pre">Node</span></code>.
Not all graphs are computation, but all graphs are composed entirely of
instances of <code class="docutils literal notranslate"><span class="pre">Node</span></code>.  Computation graphs contain only <code class="docutils literal notranslate"><span class="pre">op::Op</span></code> nodes.</p>
<p>We mostly use <a class="reference internal" href="../../glossary.html#term-shared-pointer"><span class="xref std std-term">shared pointers</span></a> for nodes, i.e.
<code class="docutils literal notranslate"><span class="pre">std::shared_ptr&lt;Node&gt;</span></code>, so that they will be automatically deallocated when
they are no longer needed. More detail on shared pointers is given in the
glossary.</p>
<p>Every node has zero or more <em>inputs</em>, zero or more <em>outputs</em>, and zero or more
<em>attributes</em>.</p>
<p>The specifics for each <code class="docutils literal notranslate"><span class="pre">type</span></code> permitted on a core <code class="docutils literal notranslate"><span class="pre">Op</span></code>-specific basis can be
discovered in our <a class="reference internal" href="../../ops/index.html"><span class="doc">List of Core ops</span></a> docs. For our purpose to
<a class="reference internal" href="#define-cmp"><span class="std std-ref">define a computation</span></a>, nodes should be thought of as essentially
immutable; that is, when constructing a node, we need to supply all of its
inputs. We get this process started with ops that have no inputs, since any op
with no inputs is going to first need some inputs.</p>
<p><code class="docutils literal notranslate"><span class="pre">op::Parameter</span></code> specifes the tensors that will be passed to the computation.
They receive their values from outside of the graph, so they have no inputs.
They have attributes for the element type and the shape of the tensor that will
be passed to them.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Build the graph</span>
    <span class="n">Shape</span> <span class="n">s</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">};</span>
    <span class="k">auto</span> <span class="n">a</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">b</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">c</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
</pre></div>
</div>
<p>The above code makes three parameter nodes where each is a 32-bit float of
shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code> and a row-major element layout.</p>
<p>To create a graph for <code class="docutils literal notranslate"><span class="pre">(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">*</span> <span class="pre">c</span></code>, first make an <code class="docutils literal notranslate"><span class="pre">op::Add</span></code> node with inputs
from <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, and an <code class="docutils literal notranslate"><span class="pre">op::Multiply</span></code> node from the add node and <code class="docutils literal notranslate"><span class="pre">c</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="k">auto</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Add</span><span class="o">&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Multiply</span><span class="o">&gt;</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>
</pre></div>
</div>
<p>When the <code class="docutils literal notranslate"><span class="pre">op::Add</span></code> op is constructed, it will check that the element types and
shapes of its inputs match; to support multiple frameworks, ngraph does not do
automatic type conversion or broadcasting. In this case, they match, and the
shape of the unique output of <code class="docutils literal notranslate"><span class="pre">t0</span></code> will be a 32-bit float with shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code>.
Similarly, <code class="docutils literal notranslate"><span class="pre">op::Multiply</span></code> checks that its inputs match and sets the element
type and shape of its unique output.</p>
<p>Once the graph is built, we need to package it in a <code class="docutils literal notranslate"><span class="pre">Function</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="k">auto</span> <span class="n">f</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">OutputVector</span><span class="p">{</span><span class="n">t1</span><span class="p">},</span>
                                        <span class="n">ParameterVector</span><span class="p">{</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">});</span>
</pre></div>
</div>
<p>The first argument to the constuctor specifies the nodes that the function will
return; in this case, the product. An <code class="docutils literal notranslate"><span class="pre">OutputVector</span></code> is a vector of references to
outputs of <code class="docutils literal notranslate"><span class="pre">op::Node</span></code>.  The second argument specifies the parameters of the
function, in the order they are to be passed to the compiled function. A
<code class="docutils literal notranslate"><span class="pre">ParameterVector</span></code> is a vector of shared pointers to <code class="docutils literal notranslate"><span class="pre">op::Parameter</span></code>.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">The parameter vector must include <strong>every</strong> parameter used in
the computation of the results.</p>
</div>
</div>
<div class="section" id="specify-the-backend-upon-which-to-run-the-computation">
<span id="specify-backend"></span><h3>Specify the backend upon which to run the computation<a class="headerlink" href="#specify-the-backend-upon-which-to-run-the-computation" title="Permalink to this headline">¶</a></h3>
<p>For a framework bridge, a <em>backend</em> is the environment that can perform the
computations; it can be done with a CPU, GPU, or <a class="reference external" href="https://www.intel.ai/nervana-nnp">purpose-built silicon</a>. A
<em>transformer</em> can compile computations for a backend, allocate and deallocate
tensors, and invoke computations.</p>
<p>Factory-like managers for classes of backend managers can compile a <code class="docutils literal notranslate"><span class="pre">Function</span></code>
and allocate backends. A backend is somewhat analogous to a multi-threaded
process.</p>
<p>There are two backends for the CPU: the optimized <code class="docutils literal notranslate"><span class="pre">&quot;CPU&quot;</span></code> backend, which uses
the <a class="reference external" href="https://intel.github.io/mkl-dnn/">DNNL</a>, and the <code class="docutils literal notranslate"><span class="pre">&quot;INTERPRETER&quot;</span></code> backend, which runs reference
versions of kernels that favor implementation clarity over speed. The
<code class="docutils literal notranslate"><span class="pre">&quot;INTERPRETER&quot;</span></code> backend can be slow, and is primarily intended for testing.
See the documentation on <a class="reference internal" href="../../backends/index.html"><span class="doc">runtime options for various backends</span></a>
for additional details.</p>
<p>To continue with our original example and select the <code class="docutils literal notranslate"><span class="pre">&quot;CPU_Backend&quot;</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Create the backend</span>
    <span class="k">auto</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="s">&quot;CPU&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="compile-the-computation">
<span id="compile-cmp"></span><h3>Compile the computation<a class="headerlink" href="#compile-the-computation" title="Permalink to this headline">¶</a></h3>
<p>Compilation triggers something that can be used as a factory for producing a
<code class="docutils literal notranslate"><span class="pre">CallFrame</span></code> which is a <em>function</em> and its associated <em>state</em> that can run
in a single thread at a time. A <code class="docutils literal notranslate"><span class="pre">CallFrame</span></code> may be reused, but any particular
<code class="docutils literal notranslate"><span class="pre">CallFrame</span></code> must only be running in one thread at any time. If more than one
thread needs to execute the function at the same time, create multiple
<code class="docutils literal notranslate"><span class="pre">CallFrame</span></code> objects from the <code class="docutils literal notranslate"><span class="pre">ExternalFunction</span></code>.</p>
</div>
<div class="section" id="allocate-backend-storage-for-the-inputs-and-outputs">
<span id="allocate-backend-storage"></span><h3>Allocate backend storage for the inputs and outputs<a class="headerlink" href="#allocate-backend-storage-for-the-inputs-and-outputs" title="Permalink to this headline">¶</a></h3>
<p>At the graph level, functions are stateless. They do have internal state related
to execution, but there is no user-visible state. Variables must be passed as
arguments. If the function updates variables, it must return the updated
variables.</p>
<p>To invoke a function, tensors must be provided for every input and every output.
At this time, a tensor used as an input cannot also be used as an output. If
variables are being updated, you should use a double-buffering approach where
you switch between odd/even generations of variables on each update.</p>
<p>Backends are responsible for managing storage. If the storage is off-CPU, caches
are used to minimize copying between device and CPU. We can allocate storage for
the three parameters and the return value.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Allocate tensors for arguments a, b, c</span>
    <span class="k">auto</span> <span class="n">t_a</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t_b</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t_c</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="c1">// Allocate tensor for the result</span>
    <span class="k">auto</span> <span class="n">t_result</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
</pre></div>
</div>
<p>Each tensor is a shared pointer to a <a class="reference internal" href="../../glossary.html#term-tensorview"><span class="xref std std-term">Tensorview</span></a>, which is the interface
backends implement for tensor use. When there are no more references to the
tensor view, it will be freed when convenient for the backend. See the
<a class="reference internal" href="../../backends/cpp-api.html"><span class="doc">Backend APIs</span></a> documentation for details on how to work
with <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
</div>
<div class="section" id="initialize-the-inputs">
<span id="initialize-inputs"></span><h3>Initialize the inputs<a class="headerlink" href="#initialize-the-inputs" title="Permalink to this headline">¶</a></h3>
<p>Next we need to copy some data into the tensors.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Initialize tensors</span>
    <span class="kt">float</span> <span class="n">v_a</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">},</span> <span class="p">{</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}};</span>
    <span class="kt">float</span> <span class="n">v_b</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">},</span> <span class="p">{</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">}};</span>
    <span class="kt">float</span> <span class="n">v_c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">},</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">}};</span>

    <span class="n">t_a</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_a</span><span class="p">));</span>
    <span class="n">t_b</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_b</span><span class="p">));</span>
    <span class="n">t_c</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_c</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_c</span><span class="p">));</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">runtime::Tensor</span></code> interface has <code class="docutils literal notranslate"><span class="pre">write</span></code> and <code class="docutils literal notranslate"><span class="pre">read</span></code> methods for
copying data to/from the tensor.</p>
</div>
<div class="section" id="invoke-the-computation">
<span id="invoke-cmp"></span><h3>Invoke the computation<a class="headerlink" href="#invoke-the-computation" title="Permalink to this headline">¶</a></h3>
<p>To invoke the function, we simply pass argument and resultant tensors to the
call frame:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Invoke the function</span>
    <span class="k">auto</span> <span class="n">exec</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="access-the-outputs">
<span id="access-outputs"></span><h3>Access the outputs<a class="headerlink" href="#access-the-outputs" title="Permalink to this headline">¶</a></h3>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">read</span></code> method to access the result:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="c1">// Get the result</span>
    <span class="kt">float</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">];</span>
    <span class="n">t_result</span><span class="o">-&gt;</span><span class="n">read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">r</span><span class="p">));</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;[&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; [&quot;</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39; &#39;</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-with-complete-shape-information">
<span id="sshp"></span><h3>Compiling with Complete Shape Information<a class="headerlink" href="#compiling-with-complete-shape-information" title="Permalink to this headline">¶</a></h3>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">“The (a + b) * c example for executing a computation on nGraph”</span><a class="headerlink" href="#id1" title="Permalink to this code">¶</a></div>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1">//*****************************************************************************</span>
<span class="c1">// Copyright 2017-2020 Intel Corporation</span>
<span class="c1">//</span>
<span class="c1">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1">// you may not use this file except in compliance with the License.</span>
<span class="c1">// You may obtain a copy of the License at</span>
<span class="c1">//</span>
<span class="c1">//     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">//</span>
<span class="c1">// Unless required by applicable law or agreed to in writing, software</span>
<span class="c1">// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1">// See the License for the specific language governing permissions and</span>
<span class="c1">// limitations under the License.</span>
<span class="c1">//*****************************************************************************</span>

<span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;ngraph/ngraph.hpp&gt;</span><span class="cp"></span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">ngraph</span><span class="p">;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="c1">// Build the graph</span>
    <span class="n">Shape</span> <span class="n">s</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">};</span>
    <span class="k">auto</span> <span class="n">a</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">b</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">c</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>

    <span class="k">auto</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Add</span><span class="o">&gt;</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Multiply</span><span class="o">&gt;</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>

    <span class="c1">// Make the function</span>
    <span class="k">auto</span> <span class="n">f</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">OutputVector</span><span class="p">{</span><span class="n">t1</span><span class="p">},</span>
                                        <span class="n">ParameterVector</span><span class="p">{</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">});</span>

    <span class="c1">// Create the backend</span>
    <span class="k">auto</span> <span class="n">backend</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="s">&quot;CPU&quot;</span><span class="p">);</span>

    <span class="c1">// Allocate tensors for arguments a, b, c</span>
    <span class="k">auto</span> <span class="n">t_a</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t_b</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">t_c</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>
    <span class="c1">// Allocate tensor for the result</span>
    <span class="k">auto</span> <span class="n">t_result</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>

    <span class="c1">// Initialize tensors</span>
    <span class="kt">float</span> <span class="n">v_a</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">},</span> <span class="p">{</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">}};</span>
    <span class="kt">float</span> <span class="n">v_b</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">},</span> <span class="p">{</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">}};</span>
    <span class="kt">float</span> <span class="n">v_c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">{{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">},</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">}};</span>

    <span class="n">t_a</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_a</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_a</span><span class="p">));</span>
    <span class="n">t_b</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_b</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_b</span><span class="p">));</span>
    <span class="n">t_c</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v_c</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">v_c</span><span class="p">));</span>

    <span class="c1">// Invoke the function</span>
    <span class="k">auto</span> <span class="n">exec</span> <span class="o">=</span> <span class="n">backend</span><span class="o">-&gt;</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
    <span class="n">exec</span><span class="o">-&gt;</span><span class="n">call</span><span class="p">({</span><span class="n">t_result</span><span class="p">},</span> <span class="p">{</span><span class="n">t_a</span><span class="p">,</span> <span class="n">t_b</span><span class="p">,</span> <span class="n">t_c</span><span class="p">});</span>

    <span class="c1">// Get the result</span>
    <span class="kt">float</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">3</span><span class="p">];</span>
    <span class="n">t_result</span><span class="o">-&gt;</span><span class="n">read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">r</span><span class="p">));</span>

    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;[&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; [&quot;</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39; &#39;</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
</div>
<div class="section" id="scenario-two-known-partial-shape">
<span id="scenario-two"></span><h2>Scenario Two: Known Partial Shape<a class="headerlink" href="#scenario-two-known-partial-shape" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#scenario-two"><span class="std std-ref">second scenario</span></a> involves the use of dynamic tensors.
A <a class="reference internal" href="../../glossary.html#term-dynamic-tensor"><span class="xref std std-term">dynamic tensor</span></a> is a tensor whose shape can change from one “iteration”
to the next. When a dynamic tensor is created, a framework <a class="reference internal" href="../../glossary.html#term-bridge"><span class="xref std std-term">bridge</span></a> might
supply only <em>partial</em> shape information: it might be <strong>all</strong> the tensor
dimensions, <strong>some</strong> of the tensor dimensions, or <strong>none</strong> of the tensor
dimensions; furthermore, the rank of the tensor may be left unspecified.
The “actual” shape of the tensor is not specified until some function writes
some value to it. The actual shape can change when the value of the tensor
is overwritten. It is the backend’s responsibility to set the actual shape.
The <a class="reference internal" href="../../glossary.html#term-model-description"><span class="xref std std-term">model description</span></a> for the second scenario based on the
<code class="file docutils literal notranslate"><span class="pre">partial_shape.cpp</span></code> code in the <code class="docutils literal notranslate"><span class="pre">/doc/examples/dynamic_tensor</span></code>
directory, and it deconstructs the steps that must happen (either
programmatically or manually) in order to successfully retreive shape data.</p>
<ul class="simple">
<li><a class="reference internal" href="#create-dyn-tensor"><span class="std std-ref">Create a dynamic tensor</span></a></li>
<li><a class="reference internal" href="#call-graph-vw"><span class="std std-ref">Initialize input of shape</span></a></li>
<li><a class="reference internal" href="#dyn-ten-result"><span class="std std-ref">Get the result</span></a></li>
<li><a class="reference internal" href="#kpsh"><span class="std std-ref">Compiling with Known Partial Shape</span></a></li>
</ul>
<p>Create and compile a graph where the provided info of shape <code class="docutils literal notranslate"><span class="pre">x</span></code> is <code class="docutils literal notranslate"><span class="pre">(2,?)</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="k">auto</span> <span class="n">x_shape_info</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="n">Dimension</span><span class="o">::</span><span class="n">dynamic</span><span class="p">()};</span>
    <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">x_shape_info</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">f</span> <span class="o">=</span> <span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">OutputVector</span><span class="p">{</span><span class="n">a</span><span class="p">},</span> <span class="n">ParameterVector</span><span class="p">{</span><span class="n">x</span><span class="p">});</span>
    <span class="k">auto</span> <span class="n">be</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="s">&quot;CPU&quot;</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">ex</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
</pre></div>
</div>
<div class="section" id="create-a-dynamic-tensor">
<span id="create-dyn-tensor"></span><h3>Create a dynamic tensor<a class="headerlink" href="#create-a-dynamic-tensor" title="Permalink to this headline">¶</a></h3>
<p>Create a dynamic tensor of shape <code class="docutils literal notranslate"><span class="pre">(2,?)</span></code></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="k">auto</span> <span class="n">t_out</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">create_dynamic_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">x_shape_info</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
<p>At this point, <code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_shape()</span></code> would throw an exception, while
<code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_partial_shape()</span></code> would return <code class="docutils literal notranslate"><span class="pre">&quot;(2,?)&quot;</span></code>.</p>
</div>
<div class="section" id="initialize-input-of-shape">
<span id="call-graph-vw"></span><h3>Initialize input of shape<a class="headerlink" href="#initialize-input-of-shape" title="Permalink to this headline">¶</a></h3>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="k">auto</span> <span class="n">t_in</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">Shape</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">});</span>
    <span class="p">{</span>
        <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">t_val</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">);</span>
        <span class="n">iota</span><span class="p">(</span><span class="n">t_val</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">t_val</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>
        <span class="n">t_in</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t_val</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t_val</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">t_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>At this point, <code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_shape()</span></code> would return <code class="docutils literal notranslate"><span class="pre">Shape{2,3}</span></code>,
while <code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_partial_shape()</span></code> would return <code class="docutils literal notranslate"><span class="pre">&quot;(2,?)&quot;</span></code>.</p>
</div>
<div class="section" id="get-the-result">
<span id="dyn-ten-result"></span><h3>Get the result<a class="headerlink" href="#get-the-result" title="Permalink to this headline">¶</a></h3>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>    <span class="n">ex</span><span class="o">-&gt;</span><span class="n">call</span><span class="p">({</span><span class="n">t_out</span><span class="p">},</span> <span class="p">{</span><span class="n">t_in</span><span class="p">});</span>

    <span class="k">auto</span> <span class="n">s</span> <span class="o">=</span> <span class="n">t_out</span><span class="o">-&gt;</span><span class="n">get_shape</span><span class="p">();</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
    <span class="n">t_out</span><span class="o">-&gt;</span><span class="n">read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;[&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; [&quot;</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39; &#39;</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>At this point, <code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_shape()</span></code> would return <code class="docutils literal notranslate"><span class="pre">Shape{2,20}</span></code>,
while <code class="docutils literal notranslate"><span class="pre">t_out-&gt;get_partial_shape()</span></code> would return <code class="docutils literal notranslate"><span class="pre">&quot;(2,?)&quot;</span></code>.</p>
</div>
<div class="section" id="compiling-with-known-partial-shape">
<span id="kpsh"></span><h3>Compiling with Known Partial Shape<a class="headerlink" href="#compiling-with-known-partial-shape" title="Permalink to this headline">¶</a></h3>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">“Full code for compiling with dynamic tensors and partial shape”</span><a class="headerlink" href="#id2" title="Permalink to this code">¶</a></div>
<div class="highlight-cpp notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1">//*****************************************************************************</span>
<span class="c1">// Copyright 2017-2020 Intel Corporation</span>
<span class="c1">//</span>
<span class="c1">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1">// you may not use this file except in compliance with the License.</span>
<span class="c1">// You may obtain a copy of the License at</span>
<span class="c1">//</span>
<span class="c1">//     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">//</span>
<span class="c1">// Unless required by applicable law or agreed to in writing, software</span>
<span class="c1">// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1">// See the License for the specific language governing permissions and</span>
<span class="c1">// limitations under the License.</span>
<span class="c1">//*****************************************************************************</span>

<span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;numeric&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;ngraph/ngraph.hpp&gt;</span><span class="cp"></span>

<span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
<span class="k">using</span> <span class="k">namespace</span> <span class="n">ngraph</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">execute</span><span class="p">(</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">&gt;</span> <span class="n">be</span><span class="p">,</span>
             <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Executable</span><span class="o">&gt;</span> <span class="n">ex</span><span class="p">,</span>
             <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">t_out</span><span class="p">,</span>
             <span class="kt">uint32_t</span> <span class="n">n</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="c1">// Create and compile a graph where the provided info of shape of x is</span>
    <span class="c1">// (2,?)</span>
    <span class="k">auto</span> <span class="n">x_shape_info</span> <span class="o">=</span> <span class="n">PartialShape</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="n">Dimension</span><span class="o">::</span><span class="n">dynamic</span><span class="p">()};</span>
    <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">make_shared</span><span class="o">&lt;</span><span class="n">op</span><span class="o">::</span><span class="n">Parameter</span><span class="o">&gt;</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">x_shape_info</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">f</span> <span class="o">=</span> <span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">OutputVector</span><span class="p">{</span><span class="n">a</span><span class="p">},</span> <span class="n">ParameterVector</span><span class="p">{</span><span class="n">x</span><span class="p">});</span>
    <span class="k">auto</span> <span class="n">be</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="s">&quot;CPU&quot;</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">ex</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>

    <span class="c1">// Create a dynamic tensor of shape (2,?)</span>
    <span class="k">auto</span> <span class="n">t_out</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">create_dynamic_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">x_shape_info</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>
    <span class="n">execute</span><span class="p">(</span><span class="n">be</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">t_out</span><span class="p">,</span> <span class="mi">20</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">execute</span><span class="p">(</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Backend</span><span class="o">&gt;</span> <span class="n">be</span><span class="p">,</span>
             <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Executable</span><span class="o">&gt;</span> <span class="n">ex</span><span class="p">,</span>
             <span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">t_out</span><span class="p">,</span>
             <span class="kt">uint32_t</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// Initialize input of shape (2, n)</span>
    <span class="k">auto</span> <span class="n">t_in</span> <span class="o">=</span> <span class="n">be</span><span class="o">-&gt;</span><span class="n">create_tensor</span><span class="p">(</span><span class="n">element</span><span class="o">::</span><span class="n">i32</span><span class="p">,</span> <span class="n">Shape</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">});</span>
    <span class="p">{</span>
        <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">t_val</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">);</span>
        <span class="n">iota</span><span class="p">(</span><span class="n">t_val</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">t_val</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="mi">0</span><span class="p">);</span>
        <span class="n">t_in</span><span class="o">-&gt;</span><span class="n">write</span><span class="p">(</span><span class="o">&amp;</span><span class="n">t_val</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t_val</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">t_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
    <span class="p">}</span>
    <span class="c1">// Get the result</span>
    <span class="n">ex</span><span class="o">-&gt;</span><span class="n">call</span><span class="p">({</span><span class="n">t_out</span><span class="p">},</span> <span class="p">{</span><span class="n">t_in</span><span class="p">});</span>

    <span class="k">auto</span> <span class="n">s</span> <span class="o">=</span> <span class="n">t_out</span><span class="o">-&gt;</span><span class="n">get_shape</span><span class="p">();</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">r</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
    <span class="n">t_out</span><span class="o">-&gt;</span><span class="n">read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]));</span>
    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;[&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; [&quot;</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39; &#39;</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;]&#39;</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="operator.html" class="btn btn-neutral float-right" title="Build a graph with operators" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Constructing Graphs" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        <span class="crt-size">&copy; Copyright 2017-2020, Intel Corporation.</span> <br/><div class="brandnote"> Intel nGraph Library contains trademarks of Intel Corporation or its subsidiaries in the U.S. and/or other countries. * Other names and brands may be claimed as the property of others; see <a href="http://ngraph.nervanasys.com/docs/latest/branding-notice.html">branding notice</a> for more information.</class>
      Last updated on Jan 29, 2020.

    </p>
  </div>
<span class="pull-right"><span class="docbws">
  Documentation built with <a href="http://sphinx-doc.org/">Sphinx</a>. Find our code on <a href="https://www.github.com/NervanaSystems/">GitHub</a>.</span></span> 
</p>
</footer>

        </div>
      </div>

    </section>

  </div>

  

 
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>